{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install All packages\n"
      ],
      "metadata": {
        "id": "36Y-BtYs7Q9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2alPqG4fy4ou",
        "outputId": "02346fe3-2a96-48b7-d44a-345da3d210bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95HmH7LEdd8K",
        "outputId": "2652fb97-65d4-467d-b82f-2ae0bbbb911d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.5/215.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain pypdf chromadb google-generativeai langchain-google-genai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import All required Packages"
      ],
      "metadata": {
        "id": "0sXWmH2N8BbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "gTOWYrwXf2IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "LC_slK5wFzm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up API key"
      ],
      "metadata": {
        "id": "MHIcvuRZ8pdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "#dotenv package to load the API key\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "jycU0VwCfHGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY') #Make secrets keys with same name as \"GOOGLE_API_KEY\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "HshOrLsXfM-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new .env file in the workspace and store the API key in it\n",
        "!echo -e 'GOOGLE_API_KEY=Paste_keys_here' > .env"
      ],
      "metadata": {
        "id": "Id0SA646i7-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()"
      ],
      "metadata": {
        "id": "XXEB_B55fetB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8abf7d4-36e7-4f2a-ea35-254b7e643104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load PDF files from Folder"
      ],
      "metadata": {
        "id": "in-2VZzv9Zp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"/content/drive/MyDrive/Colab Notebooks/pdfs\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "z0hahDgdf8Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge54_Xigg78t",
        "outputId": "47914708-8ced-459c-da34-e4214669ad22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"GENERATIVE AI WITH LLMs:  \\n \\nINTRODUCTION OF GENERATIVE AI:  \\nA novel area of artificial intelligence called generative AI aims to develop models that produce \\nfresh content in a manner similar to that of human creators. Large Language Models (LLMs), \\nsuch as the GPT series, which are well-known for their capacity to comprehend and generate \\nnatural language text, are essential to this subject.  \\nEquipped with advanced neural network architectures, these models identify patterns and \\nconnections across large datasets to produce logical and contextually appropriate language. \\nCapable of handling anything from creative text to translation, LLMs are used in a variety of \\nsectors such as text translation, text generation, text classification , text summarization , text to \\nimage generation and also u se to improve chatbot interactions and support content \\ndevelopment and analysis.  \\nPeople are working on making these models better and fairer, so they can do even more things \\nand help us in different ways. This might change how we make and use stuff in the future.  \\nOVERVIEW OF LLMs:  \\nGaining an understanding of Large Language Models entails learning how these sophisticated \\nsystems operate. They learn by analyzing vast amounts of text, much like extremely intelligent \\ncomputers. They became proficient at deciphe ring the relationships and patterns between the \\nwords in this text.  \\nUsing what they've learned, these models produce fresh content that sounds authentic —\\nalmost as if it was written by a human. Because of their extensive training on vast volumes of \\ndata, th ey are able to comprehend language and even carry out activities like providing answers \\nto queries and summarizing data.  \\nThey learn in a somewhat complicated method, deriving meaning from words and sentences \\nthrough sophisticated arithmetic and algorithms.  This aids them in producing language that \\nsounds natural and makes sense.  \\nHowever, because they are always learning from their experiences, they occasionally may make \\nmistakes or yield biased results.  \\nTRAINING AND FINE TUNING LLMs:  \\nEncouraging large volum es of data to be fed into models that educate them to comprehend \\nlanguage and produce text is known as training and fine -tuning large language models (LLMs). \\nLLMs pick up on the relationships, patterns, and structures found in this material during \\ntraining . Through this process, they learn the meanings of words and how they fit together.  \", metadata={'source': '/content/drive/MyDrive/Colab Notebooks/pdfs/ARTICLE ON GENERATIVE AI WITH LLMS.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split data into Text chunks"
      ],
      "metadata": {
        "id": "a2jf_2oI_IzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "content = \"\\n\\n\".join(str(page.page_content) for page in data)"
      ],
      "metadata": {
        "id": "hIonwQkrhgRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_text(content)"
      ],
      "metadata": {
        "id": "oUcIw59ohn6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpHwXgTghrzi",
        "outputId": "19c6f539-0141-4a4c-c38d-42838ee51bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "p_iZanP1hv4W",
        "outputId": "89fba7cc-d27a-4b84-9fe7-ea478220d845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GENERATIVE AI WITH LLMs:  \\n \\nINTRODUCTION OF GENERATIVE AI:  \\nA novel area of artificial intelligence called generative AI aims to develop models that produce \\nfresh content in a manner similar to that of human creators. Large Language Models (LLMs), \\nsuch as the GPT series, which are well-known for their capacity to comprehend and generate \\nnatural language text, are essential to this subject.  \\nEquipped with advanced neural network architectures, these models identify patterns and \\nconnections across large datasets to produce logical and contextually appropriate language. \\nCapable of handling anything from creative text to translation, LLMs are used in a variety of \\nsectors such as text translation, text generation, text classification , text summarization , text to \\nimage generation and also u se to improve chatbot interactions and support content \\ndevelopment and analysis.  \\nPeople are working on making these models better and fairer, so they can do even more things'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Embeddings model"
      ],
      "metadata": {
        "id": "UOGeZ6gh_jxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "BHrE4PWQh27_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
      ],
      "metadata": {
        "id": "vKO83B2ViBZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create embeddings for each of the Text chunks and Store them in Vectorstore-chromadb"
      ],
      "metadata": {
        "id": "I4UxjePB_5-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma.from_texts(texts, embeddings).as_retriever()"
      ],
      "metadata": {
        "id": "zcHoggHhkISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a Prompt Template"
      ],
      "metadata": {
        "id": "ULufU9vFA6bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "  Please answer the question in as much detail as possible based on the provided context.\n",
        "  Ensure to include all relevant details. If the answer is not available in the provided context,\n",
        "  kindly respond with \"The answer is not available in the context.\" Please avoid providing incorrect answers.\n",
        "\\n\\n\n",
        "  Context:\\n {context}?\\n\n",
        "  Question: \\n{question}\\n\n",
        "\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "P8O5EtjokYg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the model"
      ],
      "metadata": {
        "id": "H05iZ958BBo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             temperature=0.1)\n"
      ],
      "metadata": {
        "id": "U0dpJOMXkiH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)"
      ],
      "metadata": {
        "id": "khMcdKz_knMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is generative AI?\"\n",
        "docs = vector_store.get_relevant_documents(question)\n",
        "\n",
        "response = chain(\n",
        "    {\"input_documents\":docs, \"question\": question}\n",
        "    , return_only_outputs=True)\n",
        "# response"
      ],
      "metadata": {
        "id": "VF0a7azBkrsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "Z2Hi9FuaOPPQ",
        "outputId": "962b07b5-2c5e-4f8e-b694-65156ca6c574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Generative AI is a novel area of artificial intelligence that aims to develop models capable of producing fresh content in a manner similar to human creators. These models are equipped with advanced neural network architectures that enable them to identify patterns and connections across large datasets, allowing them to generate logical and contextually appropriate language. Generative AI systems are used in various sectors, including text translation, text generation, text classification, text summarization, text-to-image generation, and improving chatbot interactions."
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Number of foreign and domestic tourists arrivals in Gilgit-Baltistan from 2007 to 2015?\"\n",
        "docs = vector_store.get_relevant_documents(question)\n",
        "\n",
        "response = chain(\n",
        "    {\"input_documents\":docs, \"question\": question}\n",
        "    , return_only_outputs=True)\n",
        "# response"
      ],
      "metadata": {
        "id": "joptYWiFFsWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "RfShI3h2G6-L",
        "outputId": "de7cc83b-c72f-453b-e951-c9b1d8da046b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The number of foreign and domestic tourists arrivals in Gilgit-Baltistan from 2007 to 2015 is as follows:\n> \n> | Year | Total No. of Tourists | Foreign Tourists | Domestic Tourists |\n> |---|---|---|---|\n> | 2007 | 34,108 | 10,338 (30.3%) | 23,770 (69.69%) |\n> | 2008 | 62,544 | 8,504 (13.60%) | 54,040 (86.40%) |\n> | 2009 | 62,341 | 7,739 (12.42%) | 54,602 (87.58%) |\n> | 2010 | 53,028 | 7,728 (14.58%) | 45,300 (85.42%) |\n> | 2011 | 66,475 | 5,242 (7.89%) | 61,233 (92.11%) |\n> | 2012 | 33,217 | 4,324 (13.00%) | 28,893 (87.00%) |\n> | 2013 | 56,415 | 4,501 (8.00%) | 51,914 (92.00%) |\n> | 2014 | 53,746 | 3,442 (6.40%) | 50,304 (93.60%) |\n> | 2015 | 204,733 | 4,082 (2.0%) | 200,651 (98.00%) |"
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is Physicochemical test result in table 6?\"\n",
        "docs = vector_store.get_relevant_documents(question)\n",
        "\n",
        "response = chain(\n",
        "    {\"input_documents\":docs, \"question\": question}\n",
        "    , return_only_outputs=True)\n",
        "# response"
      ],
      "metadata": {
        "id": "oVFCzwiGIMfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "0liNBpP5JUWA",
        "outputId": "4fab4092-116f-4827-a130-ffb9ae74f496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The physicochemical test results in Table 6 include pH, turbidity, electric conductivity, DO, and hardness. The pH values range from 6.5 to 8.5, turbidity values range from 1.28 to 66.36 NTU, electric conductivity values range from 3.12 to 138.5 μS/cm, DO values range from 5.66 to 5.99, and hardness values range from 55.36 to 666.8 mg/L."
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is Economic impacts of tourism?\"\n",
        "docs = vector_store.get_relevant_documents(question)\n",
        "\n",
        "response = chain(\n",
        "    {\"input_documents\":docs, \"question\": question}\n",
        "    , return_only_outputs=True)\n",
        "# response"
      ],
      "metadata": {
        "id": "NDO-3cqmJb26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "mIH73spoKuyG",
        "outputId": "01bf6bf6-dea3-49be-db05-79cc03e4413d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> On the positive side, tourism is an important source of welfare in terms of being a good source of income and employment for local people. Tourism can play a role in the economic growth of a country and is also the source of foreign exchange income for developing countries, contributing to the alleviation of poverty with major fiscal activities."
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"explain PEFT?\"\n",
        "docs = vector_store.get_relevant_documents(question)\n",
        "\n",
        "response = chain(\n",
        "    {\"input_documents\":docs, \"question\": question}\n",
        "    , return_only_outputs=True)\n",
        "# response"
      ],
      "metadata": {
        "id": "LRLmUvIxKyMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "c0b0rbs3LNu4",
        "outputId": "4daecbaf-1a38-4402-e11a-8edd32969a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> PEFT (Parameter-Efficient Fine-Tuning) is a framework that integrates various techniques to enable efficient fine-tuning of large language models (LLMs) with limited resources. It focuses on adapter-based methods, which involve adding small, trainable adapters to pre-trained LLMs to reduce the number of parameters that need to be updated during fine-tuning.\n> \n> One specific adapter-based method within PEFT is LoRA (Low-Rank Adaptation), which uses low-rank matrices for efficient adaptation. PEFT also includes techniques such as 4-bit quantization, which compresses LLM weights and activations to 4 bits instead of 32 or 16 bits, dramatically reducing memory footprint and speeding up computations.\n> \n> Additionally, PEFT employs high-precision compute to preserve accuracy by performing computations using higher precision (e.g., 16-bit) to avoid quantization errors. Overall, PEFT aims to enable efficient fine-tuning of LLMs with limited resources by integrating various techniques that reduce memory requirements and preserve performance."
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5ydKereLRWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}